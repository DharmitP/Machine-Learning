# -*- coding: utf-8 -*-
"""Copy of A2_421.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JmGsDVMvCCIsoLxlGnvqpNWlRcJwRuuq
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import time
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# from google.colab import drive
# drive.mount('/content/drive')

# Load the data
def loadData():
    with np.load("notMNIST.npz") as data:
        Data, Target = data["images"], data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Implementation of a neural network using only Numpy - trained using gradient descent with momentum
def convertOneHot(trainTarget, validTarget, testTarget):
    newtrain = np.zeros((trainTarget.shape[0], 10))
    newvalid = np.zeros((validTarget.shape[0], 10))
    newtest = np.zeros((testTarget.shape[0], 10))

    for item in range(0, trainTarget.shape[0]):
        newtrain[item][trainTarget[item]] = 1
    for item in range(0, validTarget.shape[0]):
        newvalid[item][validTarget[item]] = 1
    for item in range(0, testTarget.shape[0]):
        newtest[item][testTarget[item]] = 1
    return newtrain, newvalid, newtest

def shuffle(trainData, trainTarget):
    np.random.seed(421)
    randIndx = np.arange(len(trainData))
    target = trainTarget
    np.random.shuffle(randIndx)
    data, target = trainData[randIndx], target[randIndx]
    return data, target

def relu(x):
    return np.maximum(0,x)

def softmax(x):
    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return e_x / e_x.sum(axis = 1, keepdims = True)

def computeLayer(X, W, b):
    return np.matmul(X, W) + b

def avgCE(target, prediction):
    N = target.shape[0]
    return (-1/N) * (np.sum( np.multiply(target, np.log(prediction)) ))

def gradCE(target, prediction):
    return prediction - target
    
    # N = target.shape[0]
    # np.apply_along_axis(softmax, 1, prediction)
    # return (-1/N) * np.sum(target/prediction, axis=0)

def init(input, hidden, output):    

    bias_h = np.zeros(shape=(1, hidden))
    bias_o = np.zeros(shape=(1, output))

    weight_h = np.random.normal(0, 2/(input + hidden), (input, hidden))
    weight_o = np.random.normal(0, 2/(hidden + output), (hidden, output))

    v_wh = np.full((input, hidden), 1e-5)
    v_wo = np.full((hidden, output), 1e-5)
    v_bh = np.full((1,hidden), 1e-5)
    v_bo = np.full((1,output), 1e-5)

    init_dict = {"weight_h": weight_h, "bias_h": bias_h, "weight_o": weight_o, 
                 "bias_o": bias_o, "momentum_wh": v_wh, "momentum_wo": v_wo, 
                 "momentum_bh": v_bh, "momentum_bo": v_bo}

    return init_dict

def forward_prop(data, param):
    
    param_wh = param['weight_h']
    param_bh = param['bias_h']
    param_wo = param['weight_o']
    param_bo = param['bias_o']

    interm_h = computeLayer(data, param_wh, param_bh)
    act_h = relu(interm_h)
    interm_o = computeLayer(act_h, param_wo, param_bo)
    act_o = softmax(interm_o)
    
    forward_dict = {"interm_h": interm_h, "act_h": act_h, "interm_o": interm_o, "act_o": act_o}

    return forward_dict

def backward_prop(data, labels, param, forward_dict):
    
    param_wh = param['weight_h']
    param_wo = param['weight_o']
    param_ah = forward_dict['act_h']
    param_ao = forward_dict['act_o']
    param_ih = forward_dict['interm_h']

    N = data.shape[0]

    # K x 10
    delta_o = gradCE(labels, param_ao)  # backwards message for output layer
    diff_wo = (1/N) * np.matmul(param_ah.T, delta_o)

    # 1 x 10
    ones_matrix = np.ones((N, 1))
    diff_bo = (1/N) * np.matmul(ones_matrix.T, delta_o)

    diff_relu = np.where(param_ih <= 0, 0, 1)
    delta_h = np.multiply(np.matmul(delta_o, param_wo.T), diff_relu) # backwards message for hidden layer

    diff_wh = (1/N) * np.matmul(data.T, delta_h)

    diff_bh = (1/N) * np.matmul(ones_matrix.T, delta_h)

    backward_dict = {"diff_wo": diff_wo, "diff_bo": diff_bo, "diff_wh": diff_wh, "diff_bh": diff_bh}

    return backward_dict

def training(trainData, trainLabels, validData, validLabels, testData, testLabels, hidden, epoch):
    np.random.seed(9)

    param = init(trainData.shape[1], hidden, trainLabels.shape[1])

    gamma = 0.9
    alpha = 0.2

    train_loss = []
    train_acc = []
    valid_loss = []
    valid_acc = []
    test_loss = []
    test_acc = []

    for i in range(epoch):
        # perform forward propogation and calculate loss, accuracy
        train_forward_dict = forward_prop(trainData, param)
        train_loss.append(avgCE(trainLabels, train_forward_dict['act_o']))
        train_pred = np.argmax(train_forward_dict['act_o'], axis=1)
        train_target = np.argmax(trainLabels, axis=1)
        train_acc.append(np.equal(train_pred, train_target).mean())


        valid_forward_dict = forward_prop(validData, param)
        valid_loss.append(avgCE(validLabels, valid_forward_dict['act_o']))
        valid_pred = np.argmax(valid_forward_dict['act_o'], axis=1)
        valid_target = np.argmax(validLabels, axis=1)
        valid_acc.append(np.equal(valid_pred, valid_target).mean())

        test_forward_dict = forward_prop(testData, param)
        test_loss.append(avgCE(testLabels, test_forward_dict['act_o']))
        test_pred = np.argmax(test_forward_dict['act_o'], axis=1)
        test_target = np.argmax(testLabels, axis=1)
        test_acc.append(np.equal(test_pred, test_target).mean())


        # perform backpropogation and update parameters
        grad = backward_prop(trainData, trainLabels, param, train_forward_dict)

        v_wh = param['momentum_wh']
        v_bh = param['momentum_bh']
        v_wo = param['momentum_wo']
        v_bo = param['momentum_bo']

        param_wh = param['weight_h']
        param_bh = param['bias_h']
        param_wo = param['weight_o']
        param_bo = param['bias_o']

        diff_wh = grad['diff_wh']
        diff_bh = grad['diff_bh']
        diff_wo = grad['diff_wo']
        diff_bo = grad['diff_bo']

        v_wh = (gamma * v_wh) + (alpha * diff_wh)
        v_bh = (gamma * v_bh) + (alpha * diff_bh)
        v_wo = (gamma * v_wo) + (alpha * diff_wo)
        v_bo = (gamma * v_bo) + (alpha * diff_bo)

        param_wh = param_wh - v_wh
        param_bh = param_bh - v_bh
        param_wo = param_wo - v_wo
        param_bo = param_bo - v_bo

        param = {"weight_h": param_wh, "bias_h": param_bh, "weight_o": param_wo, "bias_o": param_bo, 
                 "momentum_wh": v_wh, "momentum_bh": v_bh, "momentum_wo": v_wo, "momentum_bo": v_bo}

        #print("After iteration %i: loss: %f" %(i, avgCE(trainLabels, train_forward_dict['act_o'])))
    return train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc

trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()
trainData = trainData.reshape(trainData.shape[0], trainData.shape[1]*trainData.shape[2])
validData = validData.reshape(validData.shape[0], validData.shape[1]*validData.shape[2])
testData = testData.reshape(testData.shape[0], testData.shape[1]*testData.shape[2])

trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)
start_t = time.time()
train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = training(trainData, trainTarget, validData, validTarget, testData, testTarget, 1000, 200)
end_t = time.time()

# print time and final losses accuracies
print('Time: ' + str(end_t - start_t))
print('Final Train Loss: ' + str(train_loss[len(train_loss)-1]))
print('Final Train Accuracy: ' + str(train_acc[len(train_acc)-1]))
print('Final Validation Loss: ' + str(valid_loss[len(valid_loss)-1]))
print('Final Validation Accuracy: ' + str(valid_acc[len(valid_acc)-1]))
print('Final Test Loss: ' + str(test_loss[len(test_loss)-1]))
print('Final Test Accuracy: ' + str(test_acc[len(test_acc)-1]))

# Plot accuracies
plt.plot(train_acc, label='training accuracy')
plt.plot(valid_acc, label = 'validation accuracy')
plt.plot(test_acc, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch')
plt.show()

# Plot losses
plt.plot(train_loss, label='training loss')
plt.plot(valid_loss, label = 'validation loss')
plt.plot(test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 2.5])
plt.legend(loc='upper right')
plt.title('Losses after each Epoch')
plt.show()

### Part 1.4 Hyperparameter Investigation ###
# Hidden Units = 100
start_t = time.time()
train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = training(trainData, trainTarget, validData, validTarget, testData, testTarget, 100, 200)
end_t = time.time()

# print time and final losses accuracies
print('Time: ' + str(end_t - start_t))
print('Final Train Loss: ' + str(train_loss[len(train_loss)-1]))
print('Final Train Accuracy: ' + str(train_acc[len(train_acc)-1]))
print('Final Validation Loss: ' + str(valid_loss[len(valid_loss)-1]))
print('Final Validation Accuracy: ' + str(valid_acc[len(valid_acc)-1]))
print('Final Test Loss: ' + str(test_loss[len(test_loss)-1]))
print('Final Test Accuracy: ' + str(test_acc[len(test_acc)-1]))

# Plot accuracies
plt.plot(train_acc, label='training accuracy')
plt.plot(valid_acc, label = 'validation accuracy')
plt.plot(test_acc, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (K=100 units)')
plt.show()

# Plot losses
plt.plot(train_loss, label='training loss')
plt.plot(valid_loss, label = 'validation loss')
plt.plot(test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 2.5])
plt.legend(loc='upper right')
plt.title('Losses after each Epoch (K=100 units)')
plt.show()

# Hidden Units = 500
start_t = time.time()
train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = training(trainData, trainTarget, validData, validTarget, testData, testTarget, 500, 200)
end_t = time.time()

# print time and final losses accuracies
print('Time: ' + str(end_t - start_t))
print('Final Train Loss: ' + str(train_loss[len(train_loss)-1]))
print('Final Train Accuracy: ' + str(train_acc[len(train_acc)-1]))
print('Final Validation Loss: ' + str(valid_loss[len(valid_loss)-1]))
print('Final Validation Accuracy: ' + str(valid_acc[len(valid_acc)-1]))
print('Final Test Loss: ' + str(test_loss[len(test_loss)-1]))
print('Final Test Accuracy: ' + str(test_acc[len(test_acc)-1]))

# Plot accuracies
plt.plot(train_acc, label='training accuracy')
plt.plot(valid_acc, label = 'validation accuracy')
plt.plot(test_acc, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (K=500 units)')
plt.show()

# Plot losses
plt.plot(train_loss, label='training loss')
plt.plot(valid_loss, label = 'validation loss')
plt.plot(test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 2.5])
plt.legend(loc='upper right')
plt.title('Losses after each Epoch (K=500 units)')
plt.show()

# Hidden Units = 2000
start_t = time.time()
train_loss, train_acc, valid_loss, valid_acc, test_loss, test_acc = training(trainData, trainTarget, validData, validTarget, testData, testTarget, 2000, 200)
end_t = time.time()

# print time and final losses accuracies
print('Time: ' + str(end_t - start_t))
print('Final Train Loss: ' + str(train_loss[len(train_loss)-1]))
print('Final Train Accuracy: ' + str(train_acc[len(train_acc)-1]))
print('Final Validation Loss: ' + str(valid_loss[len(valid_loss)-1]))
print('Final Validation Accuracy: ' + str(valid_acc[len(valid_acc)-1]))
print('Final Test Loss: ' + str(test_loss[len(test_loss)-1]))
print('Final Test Accuracy: ' + str(test_acc[len(test_acc)-1]))

# Plot accuracies
plt.plot(train_acc, label='training accuracy')
plt.plot(valid_acc, label = 'validation accuracy')
plt.plot(test_acc, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (K=2000 units)')
plt.show()

# Plot losses
plt.plot(train_loss, label='training loss')
plt.plot(valid_loss, label = 'validation loss')
plt.plot(test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 2.5])
plt.legend(loc='upper right')
plt.title('Losses after each Epoch (K=2000 units)')
plt.show()