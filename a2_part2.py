# -*- coding: utf-8 -*-
"""A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lK04Vez79nfIIZAXbK6AtxnyGGgw-UR4
"""

# Commented out IPython magic to ensure Python compatibility.
# specify tensorflow version 2
# %tensorflow_version 2.x
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import time
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Load the data
def loadData():
    with np.load("notMNIST.npz") as data:
        Data, Target = data["images"], data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Implementation of a neural network using only Numpy - trained using gradient descent with momentum
def convertOneHot(trainTarget, validTarget, testTarget):
    newtrain = np.zeros((trainTarget.shape[0], 10))
    newvalid = np.zeros((validTarget.shape[0], 10))
    newtest = np.zeros((testTarget.shape[0], 10))

    for item in range(0, trainTarget.shape[0]):
        newtrain[item][trainTarget[item]] = 1
    for item in range(0, validTarget.shape[0]):
        newvalid[item][validTarget[item]] = 1
    for item in range(0, testTarget.shape[0]):
        newtest[item][testTarget[item]] = 1
    return newtrain, newvalid, newtest

# helper function to shuffle the dataset and output
def shuffle(trainData, trainTarget):
    np.random.seed(421)
    randIndx = np.arange(len(trainData))
    target = trainTarget
    np.random.shuffle(randIndx)
    data, target = trainData[randIndx], target[randIndx]
    return data, target

### Part 2.1 Model Implementation ###

# function to create convolutional neural network model
def cnn():
  # create a model by creating a linear stack of layers
  cnn_model = tf.keras.Sequential()

  # first layer is the convolution layer (also serves as the input layer)
  # takes in a batch of 28 by 28 images with 1 channel (greyscale)
  # uses 32 filters of size 3 by 3 with horizontal and vertical strides of 1
  # padding mode is 'same' so shape of output is same as input
  # after convolution, ReLU activation is done 
  # weights are intialized using Glorot Normal initializer (or Xavier)
  # biases are initialized using zeros initialization
  cnn_model.add(tf.keras.layers.Conv2D(input_shape=(28,28,1), filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', use_bias=True, kernel_initializer='glorot_normal', bias_initializer='zeros'))
  
  # second layer is the batch normalization layer
  cnn_model.add(tf.keras.layers.BatchNormalization())

  # third layer is max pooling layer using 2 by 2 pools
  # horizontal and vertical strides of 2 are used
  cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same', data_format='channels_last'))

  # fourth layer is flatten the data
  cnn_model.add(tf.keras.layers.Flatten())

  # fifth layer is a dense (fully connected) layer of 784 nodes (corresponding to each pixel)
  # weights are initialized using Glorot Normal (or Xavier)
  # biases are initialized to zeros
  # ReLU activation is done on each of the nodes
  cnn_model.add(tf.keras.layers.Dense(units=784, activation='relu', use_bias=True, kernel_initializer='glorot_normal', bias_initializer='zeros'))
  
  # sixth and last layer is another dense (fully connected) layer of 10 nodes (corresponding to each output class)
  # Softmax is done on each of the nodes to normalize into probabilities
  cnn_model.add(tf.keras.layers.Dense(units=10, activation='softmax', use_bias=True, kernel_initializer='glorot_normal', bias_initializer='zeros'))

  return cnn_model

# class definition for storing metrics of the test data set while the model is being trained
class TestMetricsCallback(tf.keras.callbacks.Callback):
    def __init__(self, test_data):
        self.test_data = test_data
        self.test_loss = []
        self.test_accuracy = []
        self.test_ce = []

    def on_epoch_end(self, epoch, logs={}):
        x, y = self.test_data
        metrics = self.model.evaluate(x, y, verbose=0)
        self.test_loss.append(metrics[0])
        self.test_accuracy.append(metrics[1])
        self.test_ce.append(metrics[2])

### Part 2.2 Model Training ###

# get data and target outputs
trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()

# convert target outputs into one hot encoding
trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)

# add a dimension to each dataset for speciying the number of channels (in this case 1: greyscale)
trainData = np.expand_dims(trainData, axis=-1) 
validData = np.expand_dims(validData, axis=-1)
testData = np.expand_dims(testData, axis=-1)

# get model
model = cnn()

# configure model by using Adam optimizer with learning rate of 1*10**-4
# use categorical cross entropy loss function
# keep history of metrics: accuracy and loss
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', 
              metrics=['accuracy', 'categorical_crossentropy'])

# train the model using training data and target outputs, shuffle before each epoch
# batch size of 32 and 50 epochs
# validate the model after each epoch
cbk = TestMetricsCallback((testData, testTarget))
history = model.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, 
                    validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk])

# Plot accuracies
plt.plot(history.history['accuracy'], label='training accuracy')
plt.plot(history.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch')
plt.show()

# Plot losses
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label = 'validation loss')
plt.plot(cbk.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch')
plt.show()

### Part 2.3 Hyperparameter Investigation ###
## 1. L2 Regularization Model ###
def cnn_L2reg(reg):
  cnn_model = tf.keras.Sequential()
  cnn_model.add(tf.keras.layers.Conv2D(input_shape=(28,28,1), filters=32, 
                                       kernel_size=(3,3), strides=(1,1), 
                                       padding='same', activation='relu', 
                                       use_bias=True, 
                                       kernel_initializer='glorot_normal', 
                                       bias_initializer='zeros'))
  cnn_model.add(tf.keras.layers.BatchNormalization())
  cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), p
                                          adding='same', data_format='channels_last'))
  cnn_model.add(tf.keras.layers.Flatten())

  # add kernel regularizer to dense layer
  cnn_model.add(tf.keras.layers.Dense(units=784, activation='relu', use_bias=True, 
                                      kernel_initializer='glorot_normal', 
                                      bias_initializer='zeros', 
                                      kernel_regularizer=tf.keras.regularizers.l2(l=reg)))

  # add kernel regularizer to dense layer
  cnn_model.add(tf.keras.layers.Dense(units=10, activation='softmax', 
                                      use_bias=True, kernel_initializer='glorot_normal', 
                                      bias_initializer='zeros', 
                                      kernel_regularizer=tf.keras.regularizers.l2(l=reg)))

  return cnn_model

## 1. L2 Regularization Investigation ##
model_l2_1 = cnn_L2reg(reg=0.01)
model_l2_2 = cnn_L2reg(reg=0.1)
model_l2_3 = cnn_L2reg(reg=0.5)

model_l2_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])
model_l2_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])
model_l2_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])

cbk_l2_1 = TestMetricsCallback((testData, testTarget))
cbk_l2_2 = TestMetricsCallback((testData, testTarget))
cbk_l2_3 = TestMetricsCallback((testData, testTarget))
history_l2_1 = model_l2_1.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk_l2_1])
history_l2_2 = model_l2_2.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk_l2_2])
history_l2_3 = model_l2_3.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk_l2_3])

# Plot accuracies
plt.plot(history_l2_1.history['accuracy'], label='training accuracy')
plt.plot(history_l2_1.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk_l2_1.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (Reg = 0.01)')
plt.show()

plt.plot(history_l2_2.history['accuracy'], label='training accuracy')
plt.plot(history_l2_2.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk_l2_2.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (Reg = 0.1)')
plt.show()

plt.plot(history_l2_3.history['accuracy'], label='training accuracy')
plt.plot(history_l2_3.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk_l2_3.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (Reg = 0.5)')
plt.show()

# Plot losses
plt.plot(history_l2_1.history['loss'], label='training loss')
plt.plot(history_l2_1.history['val_loss'], label = 'validation loss')
plt.plot(cbk_l2_1.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch (Reg = 0.01)')
plt.show()

plt.plot(history_l2_2.history['loss'], label='training loss')
plt.plot(history_l2_2.history['val_loss'], label = 'validation loss')
plt.plot(cbk_l2_2.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch (Reg = 0.1)')
plt.show()

plt.plot(history_l2_3.history['loss'], label='training loss')
plt.plot(history_l2_3.history['val_loss'], label = 'validation loss')
plt.plot(cbk_l2_3.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch (Reg = 0.5)')
plt.show()

### Part 2.3 Hyperparameter Investigation ###
## 2. Dropout Model ###
def cnn_dropout(rate):
  cnn_model = tf.keras.Sequential()
  cnn_model.add(tf.keras.layers.Conv2D(input_shape=(28,28,1), filters=32, 
                                       kernel_size=(3,3), strides=(1,1), 
                                       padding='same', activation='relu', 
                                       use_bias=True, kernel_initializer='glorot_normal', 
                                       bias_initializer='zeros'))
  cnn_model.add(tf.keras.layers.BatchNormalization())
  cnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same', 
                                          data_format='channels_last'))
  cnn_model.add(tf.keras.layers.Flatten())
  cnn_model.add(tf.keras.layers.Dense(units=784, activation='relu', use_bias=True, 
                                      kernel_initializer='glorot_normal', 
                                      bias_initializer='zeros'))

  # add dropout layer
  cnn_model.add(tf.keras.layers.Dropout(rate=rate))

  cnn_model.add(tf.keras.layers.Dense(units=10, activation='softmax', use_bias=True, 
                                      kernel_initializer='glorot_normal', 
                                      bias_initializer='zeros'))

  return cnn_model

## 2. Dropout Investigation ##
model_do_1 = cnn_dropout(rate=1-0.9)
model_do_2 = cnn_dropout(rate=1-0.75)
model_do_3 = cnn_dropout(rate=1-0.5)

model_do_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])
model_do_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])
model_do_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])

cbk_do_1 = TestMetricsCallback((testData, testTarget))
cbk_do_2 = TestMetricsCallback((testData, testTarget))
cbk_do_3 = TestMetricsCallback((testData, testTarget))
history_do_1 = model_do_1.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk_do_1])
history_do_2 = model_do_2.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk_do_2])
history_do_3 = model_do_3.fit(x=trainData, y=trainTarget, batch_size=32, epochs=50, validation_data=(validData, validTarget), shuffle=True, callbacks=[cbk_do_3])

# Plot accuracies
plt.plot(history_do_1.history['accuracy'], label='training accuracy')
plt.plot(history_do_1.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk_do_1.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (Keep Probability = 0.9)')
plt.show()

plt.plot(history_do_2.history['accuracy'], label='training accuracy')
plt.plot(history_do_2.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk_do_2.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (Keep Probability = 0.75)')
plt.show()

plt.plot(history_do_3.history['accuracy'], label='training accuracy')
plt.plot(history_do_3.history['val_accuracy'], label = 'validation accuracy')
plt.plot(cbk_do_3.test_accuracy, label='test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1])
plt.legend(loc='lower right')
plt.title('Accuracies after each Epoch (Keep Probability = 0.5)')
plt.show()

# Plot losses
plt.plot(history_do_1.history['loss'], label='training loss')
plt.plot(history_do_1.history['val_loss'], label = 'validation loss')
plt.plot(cbk_do_1.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch (Keep Probability = 0.9)')
plt.show()

plt.plot(history_do_2.history['loss'], label='training loss')
plt.plot(history_do_2.history['val_loss'], label = 'validation loss')
plt.plot(cbk_do_2.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch (Keep Probability = 0.75)')
plt.show()

plt.plot(history_do_3.history['loss'], label='training loss')
plt.plot(history_do_3.history['val_loss'], label = 'validation loss')
plt.plot(cbk_do_3.test_loss, label='test loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.title('Losses after each Epoch (Keep Probability = 0.5)')
plt.show()